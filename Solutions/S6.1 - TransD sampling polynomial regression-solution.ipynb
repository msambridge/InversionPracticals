{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 - Trans-D sampling in polynomial regression#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--<badge>--><a href=\"https://colab.research.google.com/github/msambridge/InversionPracticals/blob/main/Solutions/S6.1 - TransD sampling polynomial regression-solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><!--</badge>-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This tutorial was originally developed by Rhys Hawkins.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practical we look at Transdimensional sampling of a polynomial regression problem. That is sampling over the coefficients of a polynomial with variable order for a given two 2D $(x,y)$ dataset.\n",
    "\n",
    "Consider the 2-D data set of noisy $(x,y)$ values shown in the Figure below. Lets assume we \n",
    "are reasonably certain that a polynomial generated the data but we are uncertain\n",
    "or its order. The task is\n",
    "to recover information about the (red) function from the observations. We could perform least squares fitting for each polynomial and use some form of F-test to choose the most apprpriate order. This problem is discussed in Sambridge et al. (2006) who argue similar to previous authors that a more comprehensive solution is given by trans-dimensional Bayesian sampling.\n",
    "\n",
    "In this practical you will do this without fixing the complexity\n",
    "(polynomial order) of the curve in advance. Instead the data is used to\n",
    "constrain the number of degrees of freedom in the curve using\n",
    "(transdimensional) Bayesian sampling.\n",
    "\n",
    "<img src=\"../Figures/rjmcmc_single/ch0-exampledata.png\" alt=\"RJMCMC figure\" width=\"400\"/>\n",
    "Test 2-D data set. Red curve is the true function, dots are the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This practical makes use of Bayesian Markov chain Monte Carlo sampling\n",
    "software from the <span>*ilab*</span> inversion software\n",
    "library, available from [here](http://www.iearth.edu.au/codes/). The library is available as Fortran or C source code with a\n",
    "python interface. A PDF tutorial to the python routines is available and called\n",
    "<span>*tutorial_single.pdf*</span>\n",
    "[available here](files/Figures/tutorial_single.pdf).\n",
    "The tutorial can be used as a guide to carry out the\n",
    "following exercises with detailed descriptions of what each component does. Here you can either try and write python scripts\n",
    "yourself to carry out the exercises below, or just load the solutions written in the tutorial,\n",
    "run them and examine what they do.\n",
    "\n",
    "The task is to estimate the red curve, as well as its uncertainty using\n",
    "the Partition Modelling algorithm. This is\n",
    "implemented in the python software library <span>*rjmcmc*</span>.\n",
    "\n",
    "Lets load the trans-D python library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------- #\n",
    "#                                                          #\n",
    "#     Uncomment below to set up environment on \"colab\"     #\n",
    "#                                                          #\n",
    "# -------------------------------------------------------- #\n",
    "\n",
    "# !pip install -U anu-inversion-course\n",
    "# !git clone https://github.com/anu-ilab/JupyterPracticals\n",
    "# %cd JupyterPracticals/Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'anu_inversion_course'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m animation, rc\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01manu_inversion_course\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rjmcmc \n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'anu_inversion_course'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from matplotlib import animation, rc\n",
    "#from anu_inversion_course import rjmcmc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial we will use a non-trivial (in the sense that it will\n",
    "require a higher order polynomial to fit the function correctly) synthetic\n",
    "dataset with added noise.\n",
    "The function that is used is an exponentially increasing sine wave\n",
    "over the domain 0 . . . 10, i.e.\n",
    "\n",
    "$$y = e^{x/3} \\sin \\frac{2x}{3}$$\n",
    "\n",
    "Lets assume that the errors are independent and only in the\n",
    "y-co-ordinate and have a Gaussian distribution, with variance\n",
    "$\\sigma_i^2$. If the data are $y_i, (i=1,\\dots, n)$ and the model\n",
    "predictions at the same locations are $y_i, (i=1,\\dots, n)$, then the\n",
    "Likelihood function which measures the success of the model in fitting\n",
    "the data is given by\n",
    "\n",
    "$$p({\\bf d} | {\\bf m}) = \\frac{1}{(2\\pi)^{n/2}\\prod_{i=1}^n \\sigma_i} e ^{-\\sum_{i=1}^n [y^{obs}_i - y_i({\\bf m})]^2/\\sigma_i^2}.$$\n",
    "\n",
    "**Task 1**.  First load the given data set of $(x^{obs}_i, y^{obs}_i)$ values and\n",
    "    plot the data. You should see a figure similar to the one above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Open our data file which consists of one (x, y) coordinater per line\n",
    "# separated by whitespace\n",
    "#\n",
    "f = open('../datasets/rjmcmc_single_data.txt', 'r')\n",
    "lines = f.readlines()\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for line in lines:\n",
    "    columns = line.split()\n",
    "\n",
    "    x.append(float(columns[0]))\n",
    "    y.append(float(columns[1]))\n",
    "\n",
    "f.close()\n",
    "\n",
    "# plot the data\n",
    "fig = plt.figure()\n",
    "plt.plot(x, y, 'ko',markersize=5)\n",
    "plt.ylim(-15,10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**. In this exercise we assume a polynomial representation for the\n",
    "    unknown function (red curve) with maximum order 5 and a uniform\n",
    "    prior PDF.\n",
    "\n",
    "\n",
    "In this exercise you will need to use routines ** data = rjmcmc.dataset1d(x,y,n)** to build the data object, and ** results = rjmcmc.regression_single1d(data)** to carry out the analysis and return the results object. The parameters for this\n",
    "function are as follows with default values shown where applicable:\n",
    "\n",
    "data -> The dataset object to run the analysis on. This is an rjmcmc.\n",
    "dataset1d object which wraps the x and y vectors you load\n",
    "from the file and includes individual point noise values. This is the\n",
    "only parameter which doesn’t have a default value.\n",
    "\n",
    "burnin = 10000 The number of initial samples to throw away.\n",
    "\n",
    "total = 50000 The total number of samples to use for the analysis.\n",
    "\n",
    "max_order = 5 The maximum order of polynomial to use to fit the\n",
    "data.\n",
    "\n",
    "xsamples = 100 The number of points to sample along the x direction\n",
    "for the curve.\n",
    "\n",
    "ysamples = 100 The number of points to sample along the y directory\n",
    "for the statistics such as mode, median and confidence intervals.\n",
    "This is the number of bins for the histograms in the y direction.\n",
    "\n",
    "confidence_interval = 0.95 The confidence interval to use for minimum\n",
    "and maximum confidence intervals. This should be a value\n",
    "between 0 and 1.\n",
    "single partition regression analysis 3\n",
    "\n",
    "For this analysis we are only going to use the default values, which is implemented with ** results = rjmcmc.regression_single1d(data) **. Various properties of the results are accessible through the results object, e.g. ** results.x**. Use `help(results)' for details.\n",
    "\n",
    "Use the 1-D Partition modelling software to generate\n",
    "    50000 curves and take the mean. It should look like the figure below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Figures/rjmcmc_single/ch2-analyse.png\" alt=\"RJMCMC figure\" width=\"400\"/>\n",
    "2-D data set with mean reconstructed model from 50000 McMC samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Set up error standard deviation\n",
    "#\n",
    "sigma = 3.0\n",
    "n = [sigma] * len(x)\n",
    "\n",
    "#\n",
    "# Create the rjmcmc dataset\n",
    "#\n",
    "data = rjmcmc.dataset1d(x, y, n)\n",
    "\n",
    "#\n",
    "# rum default rjmcmc on dataset\n",
    "#\n",
    "results = rjmcmc.regression_single1d(data)\n",
    "\n",
    "#\n",
    "# Retrieve the mean curve for plotting\n",
    "#\n",
    "\n",
    "xc = results.x()\n",
    "meancurve = results.mean()\n",
    "\n",
    "#\n",
    "# Plot the data with black crosses and the mean with a red line\n",
    "#\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "#plt.plot(x, y, 'b-', label = 'Truth',linewidth=3.0)\n",
    "plt.plot(x, y, 'ko', label='Observations')\n",
    "plt.plot(xc, meancurve, 'r:', label = 'Model',linewidth=3.0)\n",
    "plt.legend()\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('MCMC fit of the observations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**  \n",
    "In this example the maximum order of the polynomial has been fixed\n",
    "    at 5. Adjust the\n",
    "    maximum order between 0 and 5 and plot the posterior distribution of\n",
    "    the order. Plot the two figures showing the mean predicted curve\n",
    "    for each case and the posterior PDF on the order of the\n",
    "    polynomial, as below. This shows how the data support has detected\n",
    "    the degree of the polynomial.\n",
    "    \n",
    " <img src=\"../Figures/rjmcmc_single/ch3-orderanalysis.png\" alt=\"RJMCMC figure\" width=\"400\"/>\n",
    " <img src=\"../Figures/rjmcmc_single/ch3-orderanalysishist.png\" alt=\"RJMCMC figure\" width=\"400\"/>\n",
    "Figure {a) (Upper panel) Mean models for 6 separate McMC runs with different maximum order of polynomial, 0 to 5; b) (Lower panel) Posterior probability distribution showing the support of the data for different polynomial orders as a function of maximum order.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run rjmcmc with differing maximum order\n",
    "\n",
    "results = []\n",
    "burnin = 500\n",
    "total = 3000\n",
    "orderlimit = 5\n",
    "for maxorder in range(orderlimit + 1):\n",
    "    print (maxorder)\n",
    "    results.append(rjmcmc.regression_single1d(data, burnin, total, maxorder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now plot results\n",
    "\n",
    "colours=['red','cyan','yellow','purple','orange','magenta']\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "#ax.plot(x, y, 'b-', label = 'Truth',linewidth=3.0)\n",
    "ax.plot(x, y, 'ko', label='Observations')\n",
    "\n",
    "# Now looping to plot the result of the RJ-MCMC analysis\n",
    "orders = []\n",
    "legendtitles = []\n",
    "for result in results:\n",
    "\n",
    "    order = result.order_histogram()\n",
    "    print(order)\n",
    "    if order == None: # The max order = 0 case will return None so \n",
    "        order = [total]\n",
    "\n",
    "    ax.plot(result.x(), result.mean(), c =colours[len(orders)], label= 'Max. Order %d' % len(orders),linewidth=2.0)\n",
    "\n",
    "    # Create the order histogram data (append zeros for orders not allowed in the analyses)\n",
    "    orders.append(order + [0] * (orderlimit + 1 - len(order)))\n",
    "\n",
    "legend = ax.legend()\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.ylim(-25,15)\n",
    "plt.title('MCMC fits with different polynomial orders')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Order analysis histogram\n",
    "\n",
    "Now we can plot a 3D bar chart showing the progression of the order histogram as the analysis maximum order is increased. The order histogram reports the count of polynomials\n",
    "of each order where the order is randomly choosen from a \n",
    "probability distribution determined by the data itself. \n",
    "\n",
    "Looking at the figure, it can be seen that when the maximum permitted\n",
    "order is 0, all the curves sampled are 0th order as expected. As this\n",
    "maximum order is increased, the distribution of orders changes until\n",
    "the step from 4 to 5 where there is little difference. It should be\n",
    "noted that if a tighter noise parameter is used, that this will change\n",
    "slightly as higher order polynomials will be more favoured to fit the\n",
    "data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = Axes3D(fig)\n",
    "fig.add_axes(ax)\n",
    "xs = range(orderlimit + 1)\n",
    "for maxorder in xs:\n",
    "    ax.bar(xs, \n",
    "           orders[maxorder], \n",
    "           zs=maxorder, \n",
    "           zdir = 'y', \n",
    "           color=colours[maxorder])\n",
    "\n",
    "ax.set_xlabel('Order')\n",
    "ax.set_ylabel('Maximum Order')\n",
    "ax.set_zlabel('Count')\n",
    "ax.set_title('Histogram of maximum orders')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence Intervals\n",
    "\n",
    "So far we have only plotted the mean of the fits, however this gives\n",
    "us no indication of distribution of the fit (this can be thought of as\n",
    "the confidence of the fit). There are a number of ways in which we can\n",
    "look at this and one of these is to look at the curves generated\n",
    "during the analysis.\n",
    "\n",
    "In this script we call a slightly different function called ** regression_single1d_sampled ** which accepts a callback function. We define this function, which accepts an x and y list which is the discretization of the current \n",
    "fitting polynomial being used. In this function we sample every\n",
    "5th polynomial and store them. \n",
    "\n",
    "** Task 4** \n",
    "Repeat the run with maximum order 5 to generate an ensemble of\n",
    "    solutions, just as in exercise 2 above, only this time use the\n",
    "    library routines to plot a density model of the entire ensemble. In\n",
    "    this way we get a visual impression of the error in the predicted\n",
    "    curve. You can use script use script <span>ch4-confidence.py</span>\n",
    "    to do this and you should get a plot similar to the figure.\n",
    "    \n",
    "<img src=\"../Figures/rjmcmc_single/ch4-confidence.png\" alt=\"RJMCMC figure\" width=\"400\"/>\n",
    "Figure  Grayscale image of probability density of all curves in the ensemble when assuming maximum polynomial order equal to 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This is an example callback function which can be passed to rjmcmc routines to sample the curves generated \n",
    "# during the analysis\n",
    "#\n",
    "sample_x = None\n",
    "sample_curves = []\n",
    "sample_i = 0\n",
    "sample_rate = 5\n",
    "def sampler_cb(x, y):\n",
    "    global sample_x, sample_curves, sample_i, sample_rate\n",
    "\n",
    "    if sample_i == 0:\n",
    "        sample_x = x\n",
    "\n",
    "    if sample_i % sample_rate == 0:\n",
    "        sample_curves.append(y)\n",
    "\n",
    "    sample_i = sample_i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Run a series of analyses with varying maximum allowed order\n",
    "#\n",
    "results = []\n",
    "burnin = 100\n",
    "total = 1000\n",
    "maxorder = 5\n",
    "results = rjmcmc.regression_single1d_sampled(data, \n",
    "                                             sampler_cb,\n",
    "                                             burnin, \n",
    "                                             total, \n",
    "                                             maxorder)\n",
    "    \n",
    "# Plot the data with black crosses, the sample curves as faint lines, and\n",
    "# the mean as a red line\n",
    "#\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "yc = 0.5\n",
    "yalpha = 1.0/((1.0 - yc) * float(len(sample_curves)))\n",
    "for sy in sample_curves:\n",
    "\n",
    "    ax.plot(sample_x, sy, \n",
    "            color = str(yc),\n",
    "            alpha = yalpha,\n",
    "            linestyle = '-',\n",
    "            linewidth = 10)\n",
    "\n",
    "ax.plot(results.x(), results.mean(), 'r-',label='Mean of models')\n",
    "ax.plot(x, y, 'ko',label='Noisy observations')\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "plt.legend()\n",
    "ax.set_title('Confidence interval (in grey)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sampled fits are plotted with transparency so that where they\n",
    "overlap this will show increased density implying that where these\n",
    "sampled polynomial ensemble appears darker, we can have higher \n",
    "confidence that the underlying function passes through that\n",
    "region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Animation\n",
    "\n",
    "Here is a script to produce an animated gif of the density cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_gif = plt.figure()\n",
    "axg = fig_gif.add_subplot(111)\n",
    "\n",
    "obs, = axg.plot(x, y, 'ko')\n",
    "line, = axg.plot([], [], 'r.',alpha = 0.05)\n",
    "\n",
    "xdata = []\n",
    "ydata = []\n",
    "\n",
    "# initialization function: plot the background of each frame\n",
    "def init():\n",
    "    obs.set_data([], [])\n",
    "    line.set_data([], [])\n",
    "    return (obs,line,)\n",
    "\n",
    "def update(i):\n",
    "    \n",
    "    xdata.append(sample_x)\n",
    "    ydata.append(sample_curves[i])\n",
    "    \n",
    "    line.set_xdata(xdata)\n",
    "    line.set_ydata(ydata)\n",
    "    \n",
    "    return line\n",
    "\n",
    "axg.set_xlabel('X')\n",
    "axg.set_ylabel('Y')\n",
    "axg.set_title('RJ-MCMC fit of noisy observations')\n",
    "\n",
    "anim2 = animation.FuncAnimation(fig_gif, update, frames=len(sample_curves), interval=100, repeat=True)\n",
    "anim2.save('MCMC_example.gif', dpi=80, writer='imagemagick')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating data noise\n",
    "\n",
    "With the hierarchical Bayesian approach we include the standard deviation\n",
    "of the noise on the observations as an unknown. In the above examples\n",
    "the noise $\\sigma$ was set to 3 units, but the actual $\\sigma$\n",
    "of the data noise in Figure 1 is 3.5.\n",
    "Can we use the data to detect the true standard deviation of its noise?\n",
    "The hierarchical Bayesian sampling scheme is implemented with the following script. Inference on the noise is implemented by \n",
    "introducing a new parameter, $\\lambda = \\frac{\\sigma_{est}}{\\sigma_{true}}$, \n",
    "defined as the ratio of the estimated noise to the real noise.\n",
    "\n",
    "** Task 5** \n",
    "Use the routines to try and estimate\n",
    "    the standard deviation of the noise in the data. In fact we invert\n",
    "    for a parameter $\\lambda$ which is the ratio of the estimated noise\n",
    "    (i.e. $\\sigma$ in eqn. 10) to the true noise. \n",
    "    \n",
    "For this exercise you will need to set the range and standard deviation of the $\\lambda$ values with the routines ** data.set_lambda_range(lambda_min, lambda_max) **, and ** data.set_lambda_std(lambda_std)**, where the data object has been obtained from the setup routine ** data = rjmcmc.dataset1d(x, y, n)**. Then you will need to perform the analysis with  ** results = rjmcmc.regression_single1d(data, pd) **, after which you can obtain various diagnostics using\n",
    "** results.partition_location_histogram(), results.partitions(), results.proposed(), results.acceptance(), results.lambda_history() **\n",
    "\n",
    "Plot a histogram of\n",
    "    the results and see how well the Bayesian sampling is able to\n",
    "    constrain the level of noise in the data. Your results should be\n",
    "    similiar to Figure below. If the data were estimated with $\\sigma=3$,\n",
    "    what do you think the true value was ?\n",
    "\n",
    "<img src=\"../Figures/rjmcmc_single/ch5-hierarchical.png\" alt=\"RJMCMC figure\" width=\"400\"/>\n",
    "Figure  Posterior PDF of the data noise parameter λ values found from sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up lambda ranges\n",
    "lambda_min = 0.5\n",
    "lambda_max = 2.0\n",
    "lambda_std = 0.05\n",
    "\n",
    "#\n",
    "# Set the lambda range\n",
    "#\n",
    "data.set_lambda_range(lambda_min, lambda_max)\n",
    "data.set_lambda_std(lambda_std)\n",
    "\n",
    "#\n",
    "# Run the default analysis\n",
    "#\n",
    "results = rjmcmc.regression_single1d(data)\n",
    "\n",
    "#\n",
    "# Retrieve the mean curve for plotting\n",
    "#\n",
    "xc = results.x()\n",
    "meancurve = results.mean()\n",
    "\n",
    "#\n",
    "# Retrieve the results of the hierarchical\n",
    "#\n",
    "p = results.proposed()\n",
    "a = results.acceptance()\n",
    "\n",
    "print ('Lambda Acceptance Rate (%):', float(a[1])/float(p[1]) * 100.0)\n",
    "\n",
    "lh = results.lambda_history()\n",
    "\n",
    "#\n",
    "# Plot the history of lambda\n",
    "#\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(range(len(lh)), lh)\n",
    "plt.xlabel('Number of runs')\n",
    "plt.xlabel('Lambda')\n",
    "plt.title('Lambda history')\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "a = plt.subplot(111)\n",
    "lsamples = lh[10000:]\n",
    "\n",
    "n, bins, patches = a.hist(lsamples, 100, range=(lambda_min, lambda_max))\n",
    "a.set_title('Figure 7: Histogram of Lambda')\n",
    "a.set_xlabel('Lambda')\n",
    "a.set_ylabel('Count')\n",
    "\n",
    "print ('Lambda average:', sum(lsamples)/float(len(lsamples)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this script we set up a uniform prior on $\\lambda$ over a pre-determined range and use\n",
    "a Gaussian distribution to perturb the $\\lambda$ values during the Markov chain.\n",
    "The range of the values of $\\lambda$ as well as the standard deviation of the\n",
    "perturbation are parameter that must be chosen. These are set in lines\n",
    "37-42. \n",
    "\n",
    "We call the function regression_single1 to do the work. \n",
    "We plot the posterior PDF of the noise $\\sigma$ as a histogram. \n",
    "This plot is shown in Figure 7.\n",
    "\n",
    "The histogram shows the support of the data for a range of $\\lambda$ values. \n",
    "Clearly there is information in the data on the likely values of noise.\n",
    "Where is the peak of the histogram? How does this compare to the ratio of the estimated\n",
    "to true $\\sigma$? Usually the ability of the data to constrain noise parameters will trade-off \n",
    "with the model complexity given, in this case, by the order of the polynomial.\n",
    "You can edit the script by changing the estimated noise and rerun to see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
